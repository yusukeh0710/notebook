{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK47l4rSAZOVnOzJkYSXxk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SSD (Single Shot Multibox Detector)\n",
        "https://arxiv.org/pdf/1512.02325"
      ],
      "metadata": {
        "id": "ZbXrWcJsr7Pt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td8iFClFr1EO",
        "outputId": "189e69fa-11d9-4012-fd23-e393f61753f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_ROOT = Path('/content/drive/MyDrive')\n",
        "\n",
        "DRIVE_DATA_ROOT = DRIVE_ROOT / 'datasets'\n",
        "DATA_ROOT= Path('./data/')\n",
        "MODEL_ROOT = DRIVE_ROOT / 'weights'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q matplotlib\n",
        "%pip install -q torchviz torchinfo\n",
        "%pip install -q tqdm"
      ],
      "metadata": {
        "id": "k6s88Hf0s4_E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchinfo import summary\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['font.size'] = 14\n",
        "plt.rcParams['figure.figsize'] = (3, 3)\n",
        "plt.rcParams['axes.grid'] = True\n",
        "np.set_printoptions(suppress=True, precision=4)\n",
        "\n",
        "# Enable GPU device if available.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "VJn2v-wYtBTO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy cached data to work directory\n",
        "def copy_cache(cache_file_path, dst_dir, chunk_size=100*(1024**2)):\n",
        "    src = Path(cache_file_path)\n",
        "    dst = Path(dst_dir) / src.name\n",
        "    if not src.exists() or dst.exists():\n",
        "        return\n",
        "\n",
        "    print(f'Cache Hit. Copying {src} to {dst}.')\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with src.open('rb') as src_file, dst.open('wb') as dst_file:\n",
        "        total_size = src.stat().st_size\n",
        "        with tqdm(total=total_size, unit='MB', unit_scale=True) as pbar:\n",
        "            while chunk := src_file.read(chunk_size):\n",
        "                dst_file.write(chunk)\n",
        "                pbar.update(len(chunk))\n",
        "\n",
        "# Copy dataset.\n",
        "copy_cache(DRIVE_DATA_ROOT/'VOCtrainval_11-May-2012.tar', DATA_ROOT)\n",
        "\n",
        "# Download backbone network's weight file.\n",
        "if not (MODEL_ROOT/'vgg16_reducedfc.pth').exists():\n",
        "    print('Downloading backbone network\\'s weight file')\n",
        "    !wget -q https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth -P {MODEL_ROOT}"
      ],
      "metadata": {
        "id": "msPAqIDvzsTA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset, Dataloader, Transformer"
      ],
      "metadata": {
        "id": "Hpn2s1eitJXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -r {DRIVE_DATA_ROOT}/utils ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1eqXUcovfcO",
        "outputId": "fb9336ec-2654-46ff-95e4-911df47e1e7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/datasets/utils': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "from utils.ssd_augmentations import (\n",
        "    Compose, ConvertFromInts, ToAbsoluteCoords, PhotometricDistort, Expand,\n",
        "    RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans\n",
        ")\n",
        "\n",
        "class ImageTransform:\n",
        "    def __init__(self, input_size, color_mean):\n",
        "        self.transform = {\n",
        "            'train': Compose([\n",
        "                ConvertFromInts(),\n",
        "                ToAbsoluteCoords(),\n",
        "                PhotometricDistort(),\n",
        "                Expand(color_mean),\n",
        "                RandomSampleCrop(),\n",
        "                RandomMirror(),\n",
        "                ToPercentCoords(),\n",
        "                Resize(input_size),\n",
        "                SubtractMeans(color_mean),\n",
        "            ]),\n",
        "            'val': Compose([\n",
        "                ConvertFromInts(),\n",
        "                Resize(input_size),\n",
        "                SubtractMeans(color_mean),\n",
        "            ]),\n",
        "        }\n",
        "\n",
        "    def __call__(self, image: PIL.Image, phase, boxes, labels):\n",
        "        bgr_image = np.array(image)[:, :, (2, 1, 0)]\n",
        "        bgr_image, boxes, labels = self.transform[phase](bgr_image, boxes, labels)\n",
        "        rgb_image = bgr_image[:, :, (2, 1, 0)]  # BGR â†’ RGB\n",
        "        return rgb_image, boxes, labels\n",
        "\n",
        "\n",
        "class AnnotationTransform:\n",
        "    def __call__(self, annotation):\n",
        "        target = []\n",
        "        for region in annotation['annotation']['object']:\n",
        "            name = region[\"name\"]\n",
        "            bbox = [int(region['bndbox'][key]) - 1 for key in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "            target.append((name, bbox))\n",
        "        return target"
      ],
      "metadata": {
        "id": "gEygdYFOwNyS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VOCDataset(Dataset):\n",
        "    CLASSES = [\n",
        "        \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
        "        \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
        "        \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\",\n",
        "    ]\n",
        "    CLASS_TO_INDEX = {\n",
        "        class_name: index for index, class_name in enumerate(CLASSES)\n",
        "    }\n",
        "    CMAP = plt.cm.hsv(np.linspace(0, 1, len(CLASSES)))\n",
        "\n",
        "    def __init__(self, root, phase, download=True, transform=None, target_transform=None):\n",
        "        self.dataset = datasets.VOCDetection(root, year='2012', image_set=phase, download=download,\n",
        "                                             target_transform=target_transform)\n",
        "        self.phase = phase\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pull_item(idx)\n",
        "\n",
        "    def pull_item(self, idx):\n",
        "        # Get inputs.\n",
        "        image, annotation = self.dataset[idx]\n",
        "        width, height = image.size\n",
        "\n",
        "        # Create normalized boxes and labels.\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for name, (xmin, ymin, xmax, ymax) in annotation:\n",
        "            boxes.append((xmin/width, ymin/height, xmax/width, ymax/height))\n",
        "            labels.append(self.CLASS_TO_INDEX[name])\n",
        "        boxes, labels = np.array(boxes), np.array(labels)\n",
        "\n",
        "        # Transform image, boxes, and labels.\n",
        "        if self.transform:\n",
        "            image, boxes, labels = self.transform(image, self.phase, boxes, labels)\n",
        "        target = np.hstack((boxes, labels.reshape(-1, 1)))\n",
        "\n",
        "        # Return as TorchTensor\n",
        "        image = transforms.ToTensor()(image)\n",
        "        target = transforms.ToTensor()(target)\n",
        "        return image, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, targets = list(zip(*batch))\n",
        "    images = torch.stack(images, dim=0)\n",
        "    return images, targets"
      ],
      "metadata": {
        "id": "dI27SfkkyS4T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SIZE = 300\n",
        "COLOR_MEAN = (104, 117, 123)\n",
        "BATCH_SIZE = 48\n",
        "\n",
        "train_dataset = VOCDataset(root=DATA_ROOT, phase='train', download=True,\n",
        "                           transform=ImageTransform(INPUT_SIZE, COLOR_MEAN),\n",
        "                           target_transform=AnnotationTransform())\n",
        "val_dataset = VOCDataset(root=DATA_ROOT, phase='val', download=True,\n",
        "                         transform=ImageTransform(INPUT_SIZE, COLOR_MEAN),\n",
        "                         target_transform=AnnotationTransform())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          collate_fn=collate_fn, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        collate_fn=collate_fn, num_workers=2)\n",
        "\n",
        "data_loader_dict = {'train': train_loader, 'val': val_loader}"
      ],
      "metadata": {
        "id": "plj3iBnQ3zFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18627f6c-384d-4193-9e2b-e54784e4e474"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
            "Extracting data/VOCtrainval_11-May-2012.tar to data\n",
            "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
            "Extracting data/VOCtrainval_11-May-2012.tar to data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Models"
      ],
      "metadata": {
        "id": "ZyozR4c20wZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(model):\n",
        "    '''Initialize conv-layer weights by He-Initialization'''\n",
        "    if isinstance(model, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(model.weight.data)\n",
        "        if model.bias is not None:\n",
        "            nn.init.constant_(model.bias.data, 0.0)"
      ],
      "metadata": {
        "id": "LHvn9xgC0rWO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_vgg(weights=None):\n",
        "    layers = []\n",
        "\n",
        "    layers += models.vgg16().features[:-1]\n",
        "    _, _, pool3_idx, pool4_idx = (i for i, layer in enumerate(layers) if isinstance(layer, nn.MaxPool2d))\n",
        "    layers[pool3_idx].ceil_mode = True\n",
        "\n",
        "    layers += [\n",
        "        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),            # pool5\n",
        "        nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),  # fc6\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(1024, 1024, kernel_size=1),                        # fc7\n",
        "        nn.ReLU(inplace=True),\n",
        "    ]\n",
        "\n",
        "    vgg = nn.Sequential(*layers)\n",
        "    if weights:\n",
        "        vgg.load_state_dict(weights)\n",
        "    else:\n",
        "        vgg.apply(init_weights)\n",
        "    return nn.ModuleList([vgg[:pool4_idx], vgg[pool4_idx:]])\n",
        "\n",
        "\n",
        "make_vgg()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP3hxyLa1BCL",
        "outputId": "b33a348d-382c-4eed-da3b-f3397438475e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "  )\n",
              "  (1): Sequential(\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "    (31): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
              "    (32): ReLU(inplace=True)\n",
              "    (33): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (34): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_extras():\n",
        "    return nn.ModuleList([\n",
        "        nn.Sequential(\n",
        "            nn.Conv2d(1024, 256, kernel_size=1),                      # conv8_1\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),  # conv8_2\n",
        "            nn.ReLU(inplace=True),\n",
        "        ),\n",
        "        nn.Sequential(\n",
        "            nn.Conv2d(512, 128, kernel_size=1),                       # conv9_1\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # conv9_2\n",
        "            nn.ReLU(inplace=True),\n",
        "        ),\n",
        "        nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=1),                       # conv10_1\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # conv10_2\n",
        "            nn.ReLU(inplace=True),\n",
        "        ),\n",
        "        nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=1),                       # conv11_1\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3),  # conv11_2\n",
        "            nn.ReLU(inplace=True),\n",
        "        ),\n",
        "    ])\n",
        "\n",
        "\n",
        "make_extras()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtiImW6F4J9r",
        "outputId": "72dacc44-472f-457c-f17a-a26b75f9dcf6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0): Sequential(\n",
              "    (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (1): Sequential(\n",
              "    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (2): Sequential(\n",
              "    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (3): Sequential(\n",
              "    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_reggressors_classifiers(num_classes, bbox_aspect_ratios):\n",
        "    reg_layers = []\n",
        "    cls_layers = []\n",
        "\n",
        "    in_channels = [512, 1024, 512, 256, 256, 256]\n",
        "    for in_channel, bbox_aspect_ratio in zip(in_channels, bbox_aspect_ratios):\n",
        "        # Conv-layer for sourceX.\n",
        "        reg_layers += [nn.Conv2d(in_channel, bbox_aspect_ratio * 4, kernel_size=3, padding=1)]\n",
        "        cls_layers += [nn.Conv2d(in_channel, bbox_aspect_ratio * num_classes, kernel_size=3, padding=1)]\n",
        "    return nn.ModuleList(reg_layers), nn.ModuleList(cls_layers)\n",
        "\n",
        "make_reggressors_classifiers(21, [4, 6, 6, 6, 4, 4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgbImhjn5SlW",
        "outputId": "a94ed89e-5bd1-4004-cb05-75417995d882"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ModuleList(\n",
              "   (0): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (1): Conv2d(1024, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (2): Conv2d(512, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (3): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (4-5): 2 x Conv2d(256, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              " ),\n",
              " ModuleList(\n",
              "   (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "   (4-5): 2 x Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              " ))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class L2Norm(nn.Module):\n",
        "    def __init__(self, in_channels=512, scale=20):\n",
        "        super(L2Norm, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.Tensor(in_channels))\n",
        "        nn.init.constant_(self.weight, scale)\n",
        "        self.eps = 1e-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt() + self.eps  # Size([num_batch, 1, 38, 38])\n",
        "        x = x / norm\n",
        "\n",
        "        # Size([512]) -> Size([num_batch, 512, 38, 38]).\n",
        "        weghts = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3),expand_as(x)\n",
        "        return x * weghts"
      ],
      "metadata": {
        "id": "1pHJh9FW6qjF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SSD(nn.Module):\n",
        "    def __init__(self, config, vgg_weights=None):\n",
        "        super(SSD, self).__init__()\n",
        "        self.num_classes = config['num_classes']\n",
        "        self.num_bbox_aspect = config['num_bbox_aspect']\n",
        "\n",
        "        # SSD Network components.\n",
        "        self.vgg = make_vgg(vgg_weights)\n",
        "        self.extras = make_extras()\n",
        "        self.L2Norm = L2Norm()\n",
        "        self.regressors, self.classifiers = make_reggressors_classifiers(\n",
        "            self.num_classes, self.num_bbox_aspect)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        sources = []\n",
        "\n",
        "        x = self.vgg[0](x)\n",
        "        source1 = self.L2Norm(x)\n",
        "        sources.append(source1)\n",
        "\n",
        "        x = self.vgg[1](x)\n",
        "        sources.append(x)\n",
        "\n",
        "        # source3 ~ source6.\n",
        "        for layer in self.extras:\n",
        "            x = layer(x)\n",
        "            sources.append(x)\n",
        "\n",
        "        loc_list = []\n",
        "        conf_list = []\n",
        "        for source, regressor, classifier in zip(sources, self.regressors, self.classifiers):\n",
        "            l = regressor(source)\n",
        "            c = classifier(source)\n",
        "\n",
        "            # Permute torch.Tensor from (N, KA, H, W) to (N, H*W*KA).\n",
        "            num_batch = loc.size(0)\n",
        "            l = l.permute(0, 2, 3, 1).contiguous().view(num_batch, -1)\n",
        "            c = c.permute(0, 2, 3, 1).contiguous().view(num_batch, -1)\n",
        "\n",
        "            loc_list.append(l)\n",
        "            conf_list.append(c)\n",
        "\n",
        "        # Concat all results.\n",
        "        loc = torch.cat(loc_list, dim=1)\n",
        "        conf = torch.cat(conf_list, dim=1)\n",
        "        return loc, conf"
      ],
      "metadata": {
        "id": "9Ie84d8b7cNM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSD_CONFIG = {\n",
        "    'num_classes': 21,  # background + 20 classes.\n",
        "    'image_size': 300,\n",
        "    'num_bbox_aspect': [4, 6, 6, 6, 4, 4],\n",
        "    'feature_map_size': [38, 19, 10, 5, 3, 1],  # Image size of each source.\n",
        "    'lr_steps': [8, 16, 32, 64, 100, 300],\n",
        "    'dbox_min_sizes': [30, 60, 111, 162, 213, 264],\n",
        "    'dbox_max_sizes': [60, 111, 162, 213, 264, 315],\n",
        "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
        "}\n",
        "\n",
        "vgg_weights = torch.load(MODEL_ROOT/'vgg16_reducedfc.pth', weights_only=False)\n",
        "model = SSD(SSD_CONFIG, vgg_weights)"
      ],
      "metadata": {
        "id": "vPd66nc39Xl_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lTuJ2F9D_W86"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}